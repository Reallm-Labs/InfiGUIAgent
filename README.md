<h1 align="center">
<img src="images/InfiGUIAgent_logo.jpg" width="100" alt="ToRA" />
<br>
InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection
</h1>

This is the repo for the paper "[InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection](https://huggingface.co/papers/2501.04575)". In this work, we develop a multimodal large language model-based GUI agent that enables enhanced task automation on computing devices. Our agent is trained through a two-stage supervised fine-tuning approach that focuses on fundamental GUI understanding skills and advanced reasoning capabilities, where we integrate hierarchical reasoning and expectation-reflection reasoning to enable native reasoning abilities in GUI interactions.

## ðŸ”¥  News
- ðŸ”¥[2025/1/9] Our paper "[InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection](https://arxiv.org/abs/2501.04575)" released.
- ðŸ”¥[2024/12/12] Our paper "[OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://os-agent-survey.github.io/)" released.
- [2024/4/2] Our paper "[InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks](https://infiagent.github.io/)" is accepted by *ICML 2024*.

## InfiGUIAgent
Data and code are coming soon.
